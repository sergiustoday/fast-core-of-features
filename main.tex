%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%  Sample document for preparing papers to  "Avtomatika i Telemekhanika"
%%  charset=utf-8
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{a&t}
\usepackage{graphicx}

\begin{document}  %%%!!!

\year{2011}
\title{НАЗВАНИЕ СТАТЬИ ИЛИ ЗАМЕТКИ БЫВАЕТ С ФОРМУЛАМИ $a+b=c$}%
\thanks{Работа выполнена при финансовой поддержке \dots
(грант \mbox{№\,\dots}).}

\authors{П.П.~ПЕРВЫЙ, д-р~техн.~наук\\
(место работы, если отличается от места работы В.В. Второго),\\
В.В.~ВТОРОЙ, канд.~физ.-мат.~наук\\
(место работы, если отличается от места работы Т.Т. Третьего),\\
Т.Т.~ТРЕТИЙ\\
(место работы, если отличается от места работы Ч.Ч. Четвертого),\\
Ч.Ч.~ЧЕТВЕРТЫЙ\\
(место работы, например, Институт проблем управления
им.~В.А.~Трапезникова РАН, Москва)}

\maketitle

\begin{abstract}
Описан сконструированный пример, показывающий, что использование лишь теоретико-информационных дефиниций и способов расчета комплементарности не позволяет ловить экранирование комплементарности, возникающее именно при последующем использовании жадного алгоритма. А при элиминировании экранирующего показателя точность модели становится выше. Предложено сконструировать алгоритм отбора показателей, учитывающий этот эффект, обоснована перспективность разработки такого алгоритма.
\end{abstract}


\section{Введение}

В машинном обучении довольно часто встречается такая ситуация, что некие два показателя по отдельности очень плохо могут предсказывать значение целевой переменной, а вместе могут предсказывать очень хорошо. Такое свойство показателей называют комплементарностью. Идея того, что показатели могут быть комлементарны друг другу, и комплементарность как таковая — это новое системное качество, не сводимое полностью к релевантности и избыточности, была известна уже давно [1, 2]. Однако лишь через несколько десятилетий после этого начали появляться алгоритмы отбора показателей, учитывающие комлементарность и за счет этого показывающие более высокие результаты [3]. В последние годы понимание того, что при отборе показателей стоит ориентироваться не на диаду «релевантность — избыточность», как было раньше [4], а на триаду «релевантность — избыточность — комплементарность» выросло и было окончательно эксплицировано [5,6,7].

Несмотря на это в крупных обзорах по отбору показателей учет комплементарности по- прежнему находит отражение достаточно неполное и краткое, иногда даже в более ранних обзорах [8] более подробное, чем в более поздних [9].
Более часто встречающаяся теоретико-информационная формализация феномена комплементарности не является единственно возможной. Существуют также формализации через ранговые корреляции [10, 11] и через построение двухъярусных деревьев решений [12, 13].хх хххх.

При перечислении можно использовать нумерованный список:
\begin{enumlist} % перечни, нумеруемые 1) 2) и т.д.
\item
ххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хх
ххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх;

\item
ххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
\end{enumlist}

Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.


\section{Заголовок второго раздела}

Более того, можно показать, что формализация феномена комплементарности через построение двухъярусного дерева в ряде случаев даже предпочтительнее, чем теоретико- информационные формализации. А именно когда после отбора показателей модель на отобранных показателях строится «жадным» алгоритмом. В этом случае «на стыке» комплементарности показателей и «жадности» алгоритма обучения модели появляется ряд новых феноменов, отсутствующих, если модель строится полнопереборным алгоритмом. Одним из таких появляющихся феноменов является возможность экранирования комплементарного эффекта взаимоусиления показателей неким третьим показателем, который будем называть экранирующим. Можно показать, что если точность этого третьего показателя будет больше точности каждого из двух комплементарных показателей по отдельности, но меньше их совместной точности, то синергетический комплементарный эффект будет заэкранирован, то есть будет построена модель меньшей точности, чем если бы этого экранирующего показателя не было бы вообще.

То есть удаление экранирующего показателя приводит к увеличению точности обученной модели. И, напротив, добавление к данным некоторого показателя может привести к уменьшению точности, если добавленный показатель окажется экранирующим. Это несколько контринтуитивно, ведь при использовании полнопереборных алгоритмов такого не возникает. Например, если к данным мы добавим новый показатель, линейно-регрессионаая модель может стать или более точной, или останется той же точности, если добавленный показатель окажется совсем не релевантным, не информативным. Но линейно-регрессионная модель не может стать менее точной при добавлении показателя. А при использовании «жадных» алгоритмов обучения это возможно, и это нужно учитывать.
Более того, можно сконструировать пример, показывающий, что использование лишь теоретико-информационных дефиниций и способов расчета комплементарности не позволяет «ловить» экранирование комплементарности, возникающее именно при последующем использовании жадного алгоритма. А при элиминировании экранирующего показателя точность модели становится выше. Это продемонстрировано на синтетическом датесете. И это открывает возможности для конструирования и всесторонней проверки алгоритма отбора показателей, учитывающего уже не вышеуказанную триаду, а квартет «релевантность — избыточность — комплементарность — экранирование комлементарности»

При перечислении можно использовать ненумерованный список:
\begin{itemlist}
\item
ххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хх
ххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх;

\item
ххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
\end{itemlist}

Введем следующее определение.

\begin{definition}
Хххххххххххххх хххххх-хххххххх х ххххх ххххххххх ххх ххххххх хххх.
Ххххх хххххххх хххххххххххххх х ххххх.
\end{definition}

Хххх ххххххх хххххххххххххх х ххххх ххххххххх ххх ххххххх хххх.

Рассмотрим следующую задачу.

\begin{problem} \label{prob:1}
Хххххх хххххххх хххххх хххххххх х ххххх ххххххххх ххх ххххххх хххх.
Ххххх хххххххх хххххххххххххх х ххххххххх х ххххх.
\end{problem}

Хххххх хххххххх хххххх хххххххх х ххххх ххххххххх ххх ххххххх хххх.
Ххххх хххххххх хххххххххххххх х ххххххххх х ххххх.


\section{Заголовок третьего раздела}

\subsection{Заголовок подраздела}

Задачей будущих исследований будет сконструировать вышеописанный алгоритм отбора показателей и проверить его точность и производительность на широком наборе синтетических и реальных наборов данных. Кроме того, отдельным направлением перспективных исследований будет изучение других феноменов, возникающих при «жадном» обучении модели на комплементарных показателях: так называемой антикомплементарности, а также синергии двух антикомплементарностей как яркого примера взаимоусиления двух взаимоослаблений.

Математический ПРИМИТИВ. Совокупность показателей.

Пусть $\mathscr{F}$ -- конечное множество показателей: $\mathscr{F} = \{f_1, ..., f_K \}, K\in\mathbb{N}$. Пусть $f \subset \mathscr{F}$ -- произвольное подмножество показателей. Тогда $\mathbb{B} (\mathscr{F})$ -- семейство всех подмножеств множества показателей $\mathscr{F}$, где $\mathbb{B}$ -- булеан-множество. Пусть $F \in B(\mathscr{F})$ -- произвольное семейство подмножеств множества $\mathscr{F}$. Таким образом, конкретные показатели будем обозначать маленькой буквой $f$ с каким-либо нижним индексом и, возможно, ещё с какими-то индексами: $f_1, f_{RM}, f^*_3, f'_i$ и так далее. Некие совокупности показателей будем обозначать маленькой буквой $f$ без нижнего индекса и, возможно, ещё с какими-то индексами: $f, f^*, f'$. Некие семейства совокупностей показателей будем обозначать большой буквой $F$, возможно, с какими-то индексами: $F, F_P, F^*_>$. 

Математический ПРИМИТИВ. Дерево. 

Определим одноярусное дерево как $t_{(1)}^i = (f_1^i, r_1^i)$. Определим двухъярусное дерево как $t_{(2)}^i = ((f_1^i, r_1^i), (f_2^i, r_2^i), (f_3^i, r_3^i)) $. Так как здесь и далее мы рассматриваем всё применительно к одному-единственному датасету, то задание показателей и отсечений по ним полностью и однозначно определяет содержимое листовых элементов. Поэтому значения в листовых элементах мы можем не указывать. 

Два дерева любой ярусности (и одноярусные, и двухъярусные) $t^1$ и $t^2$ будем называть одновершинными, если $f^1_1 = f^2_1 \land r^1_1 = r^2_1$.  То есть сравнивать по одновершинности будем и два одноярусных дерева, и два двухъярусных, и одноярусное с двухъярусным. Таким образом, зададим отношение одновершинности $\odot$ на множестве всех деревьев. 

Также будем говорить, что дерево $t_{(1)}^a = (f_1^a, r_1^a)$ содержит показатель $f^a_1$. А дерево $t_{(2)}^b = ((f_1^b, r_1^b), (f_2^b, r_2^b), (f_3^b, r_3^b)) $ содержит показатели $\{ f^b_1, f^b_2, f^b_3 \} $. Отметим, что если дерево $t$ построено на показателях $f^*$, а состоит оно из показателей $f'$, то $f' \subseteq f^*$.

Математический ПРИМИТИВ. Цепь.

Пусть у нас есть  одноярусное дерево $t_{(1)} = (f_1, r_1)$.  Тогда будем считать на нем заданными цепи $h(f_1, r_1)^R$ и $h(f_1, r_1)^L$. Пусть у нас есть двухъярусное дерево $t_{(2)} = ((f_1, r_1), (f_2, r_2), (f_3, r_3))$. Тогда будем считать на нём заданными следующие цепи: 

$h((f_1, r_1)^R, (f_2, r_2)^R)$

$h((f_1, r_1)^R, (f_2, r_2)^L)$

$h((f_1, r_1)^L, (f_3, r_3)^R)$

$h((f_1, r_1)^L, (f_3, r_3)^L)$.



ОПРЕДЕЛЕНИЯ (про ошибки и точность). 

Пусть у нас есть одноярусное дерево $t_{(1)} = (f_1, r_1)$. Тогда ошибка на нём $e(t_{(1)})$ будет складываться из ошибок цепей, ему соответствующих:

$e(t_{(1)}) = e[h(f_1, r_1)^R] + e[h(f_1, r_1)^L]$

Пусть у нас есть двухъярусное дерево $t_{(2)} = ((f_1, r_1), (f_2, r_2), (f_3, r_3))$. Тогда ошибка на нём $e(t_{(2)})$ будет складываться из ошибок цепей, ему соответствующих:

$e(t_{(2)}) = e[h((f_1, r_1)^R, (f_2, r_2)^R)] + e[h((f_1, r_1)^R, (f_2, r_2)^L)] + e[h((f_1, r_1)^L, (f_3, r_3)^R)] + e[h((f_1, r_1)^L, (f_3, r_3)^L)]$

Определим отношение точности как нестрогий порядок на множестве всех деревьев: $t^x \succeq t^y \Leftrightarrow e(t^x) \leqslant e(t^y)$, где $t^x, t^y \in T$, где $T$ -- множество всех деревьев. Отношение нестрого порядка естественным образом задает отношение строго порядка $t^x  \succ t^y$ и отношение эквивалентности $t^x \approx t^y$.  Семейство недоминируемых деревьев по отношению $\succ$ будем обозначать $T_{\succ}$.

ОПРЕДЕЛЕНИЕ. Оптимальное одноярусное дерево.

Пусть $f \in \mathbb{B}(\mathscr{F})$ -- подмножество показателей, а $T_{(1)}^f$ -- множество всех одноярусных деревьев, содержащих показатели из $f$. Пусть $T_{(1), \succ}^f$ -- недоминируемое множество всех одноярусных деревьев, содержащих показатели из $f$ по отношению точности $\succ$. Тогда определим (однозначную) функцию $G_1$, ищущую наиболее точное одноярусное дерево, как любое из (каких там?) отображений из семейства всех возможных совокупностей показателей в множество недоминируемых одноярусных деревьев: 

$G_{(1)}(f): \mathbb{B}(\mathscr{F})  \rightarrow T_{(1), \succ}^f$.

СЛЕДСТВИЕ. Очевидно, что, по определению, такой способ сопоставления совокупности показателей одноярусного дерева возвращает такое дерево, лучше которого нет во множестве всех одноярусных деревьев, содержащих не более, чем показатели $f$:

$\forall f^* \in \mathbb{B}(\mathscr{F}) \nexists \hat{t}_{(1)} \in T_{(1)}^{f^*}: \hat{t}_{(1)} \succ G_{(1)}(f^*) $.

ОПРЕДЕЛЕНИЕ. Жадный алгоритм поиска двухъярусного дерева.

Пусть $T_{(2)}^f$ -- множество всех двухъярусных деревьев, содержащих показатели из $f$. Пусть $T_{(2), \succ}^f$ -- недоминируемое множество всех двухъярусных деревьев, содержащих показатели из $f$ по отношению точности $\succ$. Пусть $T^{f, \odot}_{(2)}$ -- множество всех двухъярусных деревьев, одновершинных дереву $G_{(1)}(f)$. Пусть $T^{f, \odot}_{(2), \succ}$ недоминируемое подмножество множества $T^{f, \odot}_{(2)}$ по отношению $\succ$. Тогда определим (однозначную) функцию $G_{(2)}(f)$, ищущую наиболее точное двухъярусное дерево жадным образом, как любое из однозначных отображений из семейства всех возможных совокупностей показателей в множество  двухъярусных деревьев. Жадным образом здесь означает, что, во-первых, полученное двухъярусное дерево оказывается одновершинным оптимальному одноярусному дереву, а во-вторых, что оно является недоминируемым по отношению точности $\succ$ среди двухъярусных деревьев, одновершинных оптимальному одноярусному дереву.  

$G_{(2)}(f): \mathbb{B}(\mathscr{F})  \rightarrow T^{f, \odot}_{(2), \succ}$

СЛЕДСТВИЕ
Жадный способ получения  локально оптимального двухъярусного дерева в общем случае не гарантирует, что полученное дерево окажется недоминируемым по отношению точности $\succ$. То есть в общем случае

$G_{(2)}(f) \notin T_{(2), \succ}^f$

СЛЕДСТВИЕ

Однако, среди двухъярусных деревьев, одновершинных оптимальному одноярусному дереву, не найдется такого, точность которого была бы выше, чем точность локально оптимального двухъярусного дерева, найденного вышеописанным жадным способом.

$\forall f \in \mathbb{B}(\mathscr{F}) \nexists \hat{t}_{(2)} \in T_{(2)}^f, \hat{t}_{(2)} \odot G_{(2)}(f):  \hat{t}_{(2)} \succ G_{(2)}(f)$

Определим отношение нестрогого порядка на множестве всех совокупностей показателей: $f^x \succeq f^y$, где $f^x, f^y \subset \mathscr{F}$ или что то же самое $f^x, f^y \in \mathbb{B} ( \mathscr{F} )$. Отношение нестрого порядка естественным образом задает отношение строго порядка $f^x  \succ f^y$ и отношение эквивалентности (в предположении транзитивности отношения нестрого порядка) $f^x \approx f^y$.  Семейство недоминируемых совокупностей показателей по отношению $\succ$ будем обозначать $F_{\succ}$, понятно, что $F_{\succ} \subset \mathbb{B} (\mathscr{F})$. 

ОПРЕДЕЛЕНИЕ. Ядром показателей называется любая наиболее точная совокупность показателей, содержащая наименьшее число элементов. Формально семейство ядер $C$ для множества показателей $\mathscr{F}$ определим как

$C = \{c^{\succ} \mid c^{\succ} = \argmin_{c \in F_{\succ}} {|c|}  \}$.

ОПРЕДЕЛЕНИЕ. Комплементарность.

Пару показателей будем называть комплементарной, если дерево, построенное на этой паре показателей оказывается более точным, чем деревья, построенные на каждом из показателей в отдельности. Формально, пусть есть пара показателей $\{f_1, f_2\}$. Тогда определим бинарную (логическую?) функцию $Q(f)$, принимающую значение 1, когда два показателя комплементарны друг другу и 0 в противном случае. То есть,

$
Q(\{f_1, f_2\}) = \begin{cases}
  1,  & \mbox{если } t^{1,2}_{(2)} \succ t^2_{(2)} \land t^{1,2}_{(2)} \succ t^1_{(2)} \\
  0, & \mbox{в противном случае }
\end{cases}
$

Также будем говорить, что пара показателей $\{f_1, f_2\}$ является комлементарной. 

ОПРЕДЕЛЕНИЕ. Антикомплементарность.

Пару показателей будем называть антикомплементарной, если дерево, построенное на этой паре показателей оказывается менее точным, какое-либо из деревьев, построенных на каждом из показателей в отдельности. Формально, пусть есть пара показателей $\{f_1, f_2\}$. Тогда определим бинарную (логическую?) функцию $\widetilde{Q}(f)$, принимающую значение 1, когда два показателя антикомплементарны друг другу и 0 в противном случае. Пусть $t^1_{(2)} = G_{(2)}(\{f_1\})$, $t^2_{(2)} = G_{(2)}(\{f_2\})$ и $t^{1,2}_{(2)} = G_{(2)}(\{f_1, f_2\})$ Тогда определим, 

$
\widetilde{Q}(\{f_1, f_2\}) = \begin{cases}
  1,  & \mbox{если } t^{1,2}_{(2)} \preceq t^2_{(2)} \land t^{1,2}_{(2)} \succ t^1_{(2)} \land t^{1,2}_{(2)} \odot t^1_{(2)} \\
  0, & \mbox{в противном случае }
\end{cases}
$

Также будем говорить, что пара показателей $\{f_1, f_2\}$ является антикомлементарной.

ОПРЕДЕЛЕНИЕ

Числом комплементарных пар в тройке показателей будем называть $Q(\{f_1, f_2, f_3\}) = Q(\{f_1, f_2\}) + Q(\{f_2, f_3\}) + Q(\{f_1, f_3\})$. 

Числом антикомплементарных пар в тройке показателей будем называть $\widetilde{Q}(\{f_1, f_2, f_3\}) = \widetilde{Q}(\{f_1, f_2\}) + \widetilde{Q}(\{f_2, f_3\}) + \widetilde{Q}(\{f_1, f_3\})$. 

ТЕОРЕМА. Если в тройке показателей менее двух пар комплементарных или антикомплементарных показателей, то эта тройка не может быть ядром. Более формально, пусть $\{f_1, f_2, f_3\}$ -- тройка показателей, то есть $\{f_1, f_2, f_3\} \in  B(\mathscr{F})$. Тогда  $Q(\{f_1, f_2, f_3\}) + \widetilde{Q}(\{f_1, f_2, f_3\} < 2 \Rightarrow \{f_1, f_2, f_3\} \not\in C $.

ЛЕММА. О точности и размере ядра. 

Для любого ядра не существует такой совокупности показателей, которая была бы равна по точности нашему ядру и содержала бы меньше показателей. То есть докажем, что $\forall c \in C \,  \nexists x \in B(\mathscr{F}) : xIc \land (|x| < |c|)$.


Доказательство. Докажем лемму от противного. Предположим, что лемма не верна, тогда 

$\forall c \in C \,  \exists x \in B(\mathscr{F}) : xIc \land (|x| < |c|)$.

То есть существует такой x $\in B(\mathscr{F}) \land xIc$. Но так как по Следствию 1, $c \in F_P$, то имеем  $x \in B(\mathscr{F}) \land xIc \land c \in F_P \Rightarrow x \in F_P $  

Тогда делая соответствующую замену, имеем:

$\forall c \in C \, \exists x \in F_P : |c| > |x| $,

что прямо противоречит Следствию 2. Пришли к противоречию, значит, исходное предположение неверно, и лемма верна. Что и требовалось доказать.

ЛЕММА. О более точной парацепи. «Главная лемма».

Для того чтобы парацепь локально оптимального двухъярусного дерева, состоящая из двух показателей, была более точной, чем одновершинная ей парацепь, состоящая из одного показателя, необходимо и достаточно, чтобы эти два показателя были или комплементарными, или антикомплементарными.

Более формально, пусть есть два показателя $ f_1 \neq f_2, \, f_1, f_2 \in \mathscr{F} $ и $f_1$ -- корневой показатель локально оптимального двухъярусного дерева. Тогда

$[s((f_1^L, r_1), (f_2, r_2)) \succ s((f_1^L, r_1), (f_1, r'_1))] \lor [s((f_1^R, r_1), (f_2, r'_2)) \succ s((f_1^R, r_1), (f_1, r''_1))] \Leftrightarrow Q(\{f_1, f_2 \}) \lor \widetilde{Q}(\{f_1, f_2 \}) $.


Доказательство. Сначала переберем все варианты двухъярусных деревьев решений, построенных на двух показателях. И будем отмечать, в каком из случаев мы имеем комплементарную пару показателей, в каком антикомплементарную, а когда пара ни комплементарна, ни антикомплементарна. И проследим получаются ли в каждом из этих случаев более точные цепочки или нет. Эти перебранные варианты мы используем дальше для доказательства необходимости и достаточности. 

Рассмотрим все двуъярусные деревья решений, построенные на двух показателях. Для этого рассмотрим размещения с повторениями из двух по три, так как показателя два, а мест, куда они могут разместиться в составе сплитов в двухъярусном дереве три. Мы знаем, что число размещений с повторениями вычисляется по формуле $ \overline{A}_n^k = n^k $, для нашего случая $ n = 2, k = 3, \overline{A}_2^3 = 2^3 = 8 $.  Если $ f_1$ и $f_2 $ -- показатели, то все варианты их размещения в сплитах двухъярусного дерева можно описать как $ \{t_1 = (f_1, f_1, f_1), \, t_2 = (f_1, f_1, f_2), \, t_3 = (f_1, f_2, f_1), \, t_4 = (f_1, f_2, f_2), \, t_5 = (f_2, f_1, f_1), \, t_6 = (f_2, f_1, f_2), \, t_7 = (f_2, f_2, f_1), \, t_8 = (f_2, f_2, f_2)  \} $ . Два из этих деревьев содержат только один показать -- $t_1$ и $t_8$, остальные содержат по два показателя. 

Рассмотрим все возможные сочетания деревьев, построенных на одном показателе и на двух показателях. На показателе $f_1$, очевидно, может быть построено только дерево $t_1$, на показателе $f_2$ дерево $t_8$. На паре показателей $\{f_1, f_2 \}$ в общем случае деревья $ \{t_1, \ldots , t_8 \} $. Переберем все эти восемь случаев. 

Предположим, что на паре показателей $ \{ f_1, f_2 \} $ было построено дерево $t_1$. Поскольку мы перебираем эти варианты в контексте того, будут ли образованы более точные цепочки или нет, здесь вообще новых цепочек не образуется, поэтому вопрос о точности отпадает сам собой. Аналогично и в случае, если на паре показателей строится дерево $t_8$. Рассмотрим остальные варианты. 

Предположим, что на паре показателей $ \{ f_1, f_2 \} $ было построено дерево $t_2$. 

$t_2.1)$ Тогда рассмотрим случаи, когда $t_1 P^T t_8$. 

$t_2.1.1)$ $t_1 P^T t_8 I^T t_2 \Rightarrow t_1 P^T t_2$, что невозможно.

$t_2.1.2)$ $t_1 P^T t_8 P^T t_2 \Rightarrow t_1 P^T t_2$, что невозможно.

$t_2.1.3)$ $t_1 I^T t_2 P^T t_8 \Rightarrow t_1 I^T t_2 \Rightarrow (f_1^R, f_2)I(f_1^R, f_1) $, при этом по определению пара $(f_1, f_2)$ не является ни комплементарной, ни антикомплементарной.

$t_2.1.4)$ $t_2 P^T t_1 P^T t_8 \Rightarrow t_2 P^T t_1 \Rightarrow (f_1^R, f_2)P(f_1^R, f_1) $, при этом по определению пара $(f_1, f_2)$ является комплементарной.

$t_2.2)$ Рассмотрим случаи, когда $t_1 I^T t_8$. 

$t_2.2.1)$ $t_1 I^T t_8 I^T t_2 \Rightarrow t_1 I^T t_2 \Rightarrow (f_1^R, f_2)I(f_1^R, f_1) $, при этом по определению пара $(f_1, f_2)$ не является ни комплементарной, ни антикомплементарной.

$t_2.2.2)$ $t_2 P^T t_8 I^T t_1 \Rightarrow t_2 P^T t_1 \Rightarrow (f_1^R, f_2)P(f_1^R, f_1) $, при этом по определению пара $(f_1, f_2)$ является комплементарной.

$t_2.3)$ Рассмотрим случаи, когда $t_8 P^T t_1$. 

$t_2.3.1)$ $t_8 P^T t_2 I^T t_1 \Rightarrow t_1 I^T t_2 \Rightarrow (f_1^R, f_2)I(f_1^R, f_1) $, при этом по определению пара $(f_1, f_2)$ не является ни комплементарной, ни антикомплементарной.

$t_2.3.2)$ $t_8 P^T t_2 P^T t_1 \Rightarrow (f_1^R, f_2)P(f_1^R, f_1) $, при этом по определению пара $(f_1, f_2)$ является антикомплементарной.

$t_2.3.3)$ $t_8 I^T t_2 P^T t_1 \Rightarrow (f_1^R, f_2)P(f_1^R, f_1) $, при этом по определению пара $(f_1, f_2)$ является антикомплементарной.

$t_2.3.4)$ $t_2 P^T t_8 P^T t_1 \Rightarrow (f_1^R, f_2)P(f_1^R, f_1) $, при этом по определению пара $(f_1, f_2)$ является комплементарной.

Случаи с деревом $t_3$ анализируются так же, как и с деревом $t_2$ ввиду их симметрии с точностью до замены правых цепей левыми.

Проанализируем случаи с деревом $t_4$. ДОПИСАТЬ!!!

Легко видеть, что случаи с деревьями, где в корневой вершине находится сплит по показателю $f_2$ аналогичен уже рассмотренным случаям с $f_1$в корне, поэтому случаи с деревьями $t_5, \ldots , t_8$приводят к тем же результатам.  

Необходимость. Для того чтобы была создана более точная цепь на двух показателях, необходимо, чтобы эти два показателя были или комплементарными, или антикомплементарными. Более формально, $(f_1^L, f_2)P(f_1^L, f_1) \lor (f_1^R, f_2)P(f_1^R, f_1) \Rightarrow Q(\{f_1, f_2 \}) \lor \widetilde{Q}(\{f_1, f_2 \}) $, где $ f_1 \neq f_2, \, f_1, f_2 \in \mathscr{F} $. По правилу контрапозиции получается, что если эти два показателя не комплементарны и не антикомплементарны, то цепочка на них не будет более точной. Более формально, $ \overline{Q(\{f_1, f_2 \}) \lor \widetilde{Q}(\{f_1, f_2 \})} \Rightarrow \overline{(f_1^L, f_2)P(f_1^L, f_1) \lor (f_1^R, f_2)P(f_1^R, f_1)} $. Или же $ \overline{Q(\{f_1, f_2 \})} \land \overline{ \widetilde{Q}(\{f_1, f_2 \})} \Rightarrow \overline{(f_1^L, f_2)P(f_1^L, f_1)} \land \overline{(f_1^R, f_2)P(f_1^R, f_1)} $ . Пара показателей $(f_1, f_2)$ является и не комплементарной, и не антикомплементарной в случаях $\{ t_2.1.3, t_2.2.1, t_2.3.1 \}$. Во всех этих случаях более точных цепей не формируется. Необходимость доказана.

Достаточность.  Для того чтобы была создана более точная цепь на двух показателях, достаточно, чтобы эти два показателя были или комплементарными, или антикомплементарными. То есть, если эти два показателя или комплементарны, или антикомплементарны, то цепочка на них не будет более точной.  Более формально, $ Q(\{f_1, f_2 \}) \lor \widetilde{Q}(\{f_1, f_2 \}) \Rightarrow (f_1^L, f_2)P(f_1^L, f_1) \lor (f_1^R, f_2)P(f_1^R, f_1) $, где $ f_1 \neq f_2, \, f_1, f_2 \in \mathscr{F} $. Пара показателей $(f_1, f_2)$ является или комплементарной, или антикомплементарной в случаях $\{ t_2.1.4, \, t_2.2.2, \, t_2.3.2, \, t_2.3.3, \, t_2.3.4 \}$. Во всех этих случаях формируется хотя бы одна более точная цепочка. Достаточность, а вместе с ней и лемма в целом доказаны.

ЛЕММА. О следствии отсутствия комплементарно-антикомплементарных пар.

Есть тройка показателей, никакие два из них не являются ни комплементарными, ни антикомплементарными. Тогда эта тройка не может быть ядром. Более формально, $[ Q(\{f_1, f_2, f_3\}) + \widetilde{Q}(\{f_1, f_2, f_3\} = 0] \Rightarrow \{f_1, f_2, f_3\} \not\in C  $


Доказательство. Пусть на тройке показателей $ \{f_1, f_2, f_3\} $ построено двухъярусное дерево $t$. Если оно содержит один или два показателя, то оно совпадает с неким деревом, построенном на одном или двух показателям, соответственно, и по точности тоже совпадает. А раз есть дерево, не меньшее по точности, но меньшее по числу показателей, то первое дерево не может быть ядром (по лемме).

Тогда предположим, что на тройке показателей $ \{f_1, f_2, f_3\} $ построено двухъярусное дерево $t$, содержащее три показателя: $ t = ((f_1, h_1), (f_2, h_2), (f_3, h_3))$. Так как пара показателей $\{ f_1, f_2 \}$ не является ни комплементарной, ни антикомплементарной, то правая цепочка из двух показателей равна цепочке из одного показателя: $(f_1^R, f_2)I(f_1^R, f_1)$. Так как пара показателей $\{ f_1, f_3 \}$ не является ни комплементарной, ни антикомплементарной, то левая цепочка из двух показателей равна цепочке из одного показателя: $(f_1^L, f_3)I^C(f_1^L, f_1)$. Заметим, что из $(f_1^R, f_2)I^C(f_1^R, f_1) \land (f_1^L, f_3)I^C(f_1^L, f_1)$ следует, что $t = T(\{f_1, f_2, f_3\}) \, I^T \, T(\{f_1\}) $.

То есть, если в тройке показателей никакие три не являются ни комплементарными, ни антикомплементарными, то точность этой тройки равна по точности дереву, построенному на каком-то одном из этих трех показателей. А раз есть дерево, не меньшее по точности, но меньшее по числу показателей, то первое дерево не может быть ядром (по лемме).


ЛЕММА. Об одной комплементарно-антикомплементарной паре

Есть тройка показателей, и только одна пара там или комплементарна, или антикомплементарна. Тогда эта тройка не может быть ядром. Более формально, $[ Q(\{f_1, f_2, f_3\}) + \widetilde{Q}(\{f_1, f_2, f_3\} = 1] \Rightarrow \{f_1, f_2, f_3\} \not\in C $.


Доказательство. Если в тройке показателей только одна пара является комплементарной, то такая тройка будет равна по точности дереву, построенному на этой паре. А если антикомплементарной, то равна по точности дереву, построенному на одном показателе.  А раз есть дерево, не меньшее по точности, но меньшее по числу показателей, то первое дерево не может быть ядром (по лемме)

Сформулируем следующую теорему.

\begin{theorem}[{\cite[c.\,123]{first}}] %
Пусть выполнены следующие условия:

\begin{ruslist}
\item
первое условие;

\item
второе условие.
\end{ruslist}

Тогда справедливы следующие утверждения:

\begin{enumlist}
\item
первое утверждение;

\item
второе утверждение.
\end{enumlist}
\end{theorem}

\begin{proof}
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх:
\begin{gather}
    2\times 2=4.
\end{gather}
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
\end{proof}

Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.

\begin{corollary}
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Хххххххххххххх хххххххх х хххххххххххххх ххх ххххххх хххх.
\end{corollary}

Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.


\subsection{Заголовок следующего подраздела}

Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.

\begin{lemma}[(см.\ {\cite[c.\,45]{second}})] \label{lm:1}
Хххххххххххххх хххххххххххххх х хххххххххх:
\begin{multline}
    2\times 2\times 2\times 2\times 2\times 2\times 2\times 2\times
    2\times 2\times 2\times 2\times 2\times 2\times 2\times 2\times
\\
    \times 2\times 2\times 2\times 2\times 2\times 2\times 2\times 2\times
    2\times 2\times 2\times 2\times 2\times 2\times 2\times 2
\end{multline}
хххххххх ххххххх ххххххх х хххххххххххххх ххх.
\end{lemma}

Опишем следующий алгоритм.

\begin{algorithm}[(Быстрый)] \label{alg:1}
\ %%<-- этот пробел для того, чтобы первый элемент перечня был
%% на следующей строке, а не в подбор к заголовку окружения

\begin{enumlist}[.] % перечни, нумеруемые 1. 2. и т.д.
% \setcounter{enumlisti}{-1} % <-- эта команда нужна
%% для нумерации элементов перечня с нулевого
\item
Начать.

\item
Изменить.

\item
Закончить.
\end{enumlist}
\end{algorithm}

Следующая теорема утверждает сходимость алгоритма~\ref{alg:1}.

\begin{theorem}[(Теорема сходимости)] \label{th:2}
Алгоритм~{\rm\ref{alg:1}} сходится.
\end{theorem}

Можно привести замечание.

\begin{remark}
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
\end{remark}

\section{Четвертый раздел}

Данный раздел содержит несколько примеров различных окружений.

\begin{example}
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Хххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.

Некоторые перечни можно нумеровать русскими буквами:
\begin{ruslist} % перечни, нумеруемые а), б) и т.д.
\item
ххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хх
ххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх;

\item
ххххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
\end{ruslist}

А некоторые можно~"--- и латинскими буквами:

\begin{latlist} % перечни, нумеруемые а), б) и т.д.
\item
ххххххххххх ххх ххххххх хххх;

\item
хххххххххххх ххх ххххххх хххх.
\end{latlist}
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
\end{example}

Можно сформулировать утвеждение.

\begin{statement}
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
\end{statement}

Можно сформулировать предложение.

\begin{proposition}
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
Ххххххххххх хххххххххххххх х хххххххххххххх ххх ххххххх хххх.
\end{proposition}

\appendix{1}  % приложения можно нумеровать, если их несколько

\begin{proofoftheorem}{\ref{th:2}}
Докажем сначала, что выполнено следующее соотношение:
\begin{gather} \label{f(x)=y}
    f(x)=y.
\end{gather}
Действительно,~\dots откуда получаем, что
равенство~\eqref{f(x)=y} справедливо. Следовательно,~\dots
окончательно,~\dots

Теорема~\ref{th:2} доказана.
\end{proofoftheorem}

\begin{proofoflemma}{\ref{lm:1}}
Очевидно, что \dots Таким образом, \dots, что и требовалось
доказать.
\end{proofoflemma}


\begin{thebibliography}{10}



\bibitem{first}
{\it Первый П.П.}
Статья в журнале // АиТ. 2011. № 13. С. 1--50.

\bibitem{second}
{\it Второй В.В., Первый П.П.}
Монография по данной теме. М.: Высш. шк., 1999.

\bibitem{third}
{\it Третий Т.Т.}
Публикация в трудах конференции //
Тр. Ин-та такого-то РАН. 2000. Т. 1. № 2. С. 3--4.

\bibitem{forth}
{\it Четвертый Ч.Ч.}
Публикация по теме в серийном издании или сборнике /
Сб. научн. тр. к 20-летию Института. Новосибирск, Изд-во <<Пирогов>>, 2000. Т. 1. № 2. С. 3--4.

\bibitem{fifth}
{\it Fifth F.}
Some journal publication in English //
Appl. Math. Comput. J., Elsevier Publ. 1925. V. 501. No. 1. P. 1234--5678.

\bibitem{sixth}
{\it Sixth J.Th.}
A book in English. Boston: Springer, 1991.

\end{thebibliography}

\AdditionalInformation{Фамилия И.О. первого автора}{место
работы, должность, город}{адрес электронной почты}

\AdditionalInformation{Фамилия И.О. второго автора}{место
работы, должность, город}{адрес электронной почты}

\AdditionalInformation{Фамилия И.О. третьего автора}{место
работы, должность, город}{адрес электронной почты}

\AdditionalInformation{Фамилия И.О. четвертого автора}{место
работы, должность, город}{адрес электронной почты}

\AdditionalInformation{Фамилия И.О. пятого автора}{место
работы, должность, город}{адрес электронной почты}


R. Kohavi and G. H. John. Wrappers for feature subset selection. Artificial Intelligence, 97(1-2):273–324, 1997.
2. I. Guyon and A. Elisseeff. An introduction to variable and feature selection. Journal of Machine Learning Research, 3:1157–1182, 2003.
3. Meyer P. E., Schretter C., Bontempi G. Information-theoretic feature selection in microarray data using variable complementarity //IEEE Journal of Selected Topics in Signal Processing. – 2008. – Т. 2. – No. 3. – С. 261-274.
4. Yu L., Liu H. Efficient feature selection via analysis of relevance and redundancy // The Journal of Machine Learning Research. – 2004. – Т. 5. – С. 1205-1224.
5. Shishkin, A., Bezzubtseva, A., Drutsa, A., Shishkov, I., Gladkikh, E., Gusev, G., & Serdyukov, P. Efficient high-order interaction-aware feature selection based on conditional mutual information // Proceedings of the 30th International Conference on Neural Information Processing Systems. – 2016. – С. 4644-4652.
6. Singha S., Shenoy P. P. An adaptive heuristic for feature selection based on complementarity //Machine Learning. – 2018. – Т. 107. – No. 12. – С. 2027-2071.
7. Li, C., Luo, X., Qi, Y., Gao, Z., & Lin, X. A new feature selection algorithm based on relevance, redundancy and complementarity // Computers in biology and medicine. – 2020. – Т. 119. – С. 103667.
8. Kotsiantis S. Feature selection for machine learning classification problems: a recent overview // Artificial Intelligence Review. – 2011. – Т. 42. – No. 1. – С. 157-176.
9. Li, J., Cheng, K., Wang, S., Morstatter, F., Trevino, R. P., Tang, J., & Liu, H. Feature selection: A data perspective // ACM Computing Surveys (CSUR). – 2017. – Т. 50. – No. 6. – С. 1-45.
Салтыков С.А. Algorithm of Building Regression Decision Tree Using Complementary Features / Proceedings of the 13th International Conference "Management of Large-Scale System Development" (MLSD). М.: IEEE, 2020. С. https://ieeexplore.ieee.org/document/ 9247785. DOI: 10.1109/MLSD49919.2020.9247785.
11. Салтыков С.А. Корреляция наукометрических показателей из РИНЦ с цитированием по базе Web of Science / Труды 13-й Международной конференции «Управление развитием крупномасштабных систем» (MLSD'2020, Москва). М.: ИПУ РАН, 2020. С. 1677-1684.
12. Салтыков С.А. Analysis of Decrease in Accuracy of Two-tier Trees without Using Feature Selection / Proceedings of the 14th International Conference "Management of Large-Scale System Development" (MLSD). Moscow: IEEE, 2021. С. https://ieeexplore.ieee.org/ document/9600173. DOI: 10.1109/MLSD52249.2021.9600173.
13. Салтыков С.А. Алгоритм отбора показателей для построения двухъярусного дерева решений в задачах объясняемого искусственного интеллекта / Труды 14-й Международной конференции "Управление развитием крупномасштабных
систем" (MLSD-2021). М.: ИПУ РАН, 2021. С. 1544-1551.


\end{document}
